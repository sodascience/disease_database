{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the API of Delpher for downloading newspaper content on page level based\n",
    "\n",
    "\n",
    "This notebook guides you through the endpoints of the KB: National Library of the Netherlands\n",
    "\n",
    "- The SRU (Search and Retrieval via URL) endpoint allows you to search newspaper issues and articles. \n",
    "- The OAI-PMH endpoint is a protocol for harvesting metadata.\n",
    "- The resolver endpoint provides the actual content, like transcripts (OCR) and images (scans).\n",
    "\n",
    "... in order to collect content. Content can be collected either on the level of the entire newspaper issue, or on page level.\n",
    "\n",
    "The starting point for this tutorial is the *ppn* of a newspaper. A ppn is the identifier of a (newspaper) title in the library catalogue. You can find a list of newspaper titles and corresponding links through this link: https://www.kb.nl/kbhtml/delpher/documentatie/beschikbare_kranten_alfabetisch.pdf. Please note that one newspaper title can have several PPNâ€™s, due to name changes, precursors and successors of titles, supplements or special editions.\n",
    "\n",
    "As an example, we use the newspaper 'Trouw' with the ppn '412789353' from 01 january 1970 until 31 december  1970. \n",
    "\n",
    "--- PDF verwijderen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the neccesary packages\n",
    "\n",
    "It is preffered to install the package through a commandline, but installing through the Jupypter Notebook is also possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not already installed, install the following packages\n",
    "!pip install pandas\n",
    "!pip install requests\n",
    "!pip install BeautifulSoup4\n",
    "!pip install six"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import  the neccesary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from six.moves import urllib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the api key\n",
    "\n",
    "For accessing post-1945 material, you'll need an api key which can be requested through our Dataservices department,\n",
    "via dataservices@kb.nl. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apikey = \"insert api key\" ## Insert the recieved APIkey between the quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the search parameters\n",
    "\n",
    "There are various parameters that can be used to search through the selections.\n",
    "The code in this notebook is based on searching with a ppn (an identifier of a newspaper title) as entrypoint. \n",
    "\n",
    "Furthermore, other parameters can be searched, such as the desired start and end date, the type of content or the geographical area. In this notebook, we use only the ppn and the date range. Reference the user manual (available upon request at dataservices@kb.nl) for more information about the other parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert required variables\n",
    "ppn = \"412789353\" ## Example ppn of 'de Trouw' newspaper, please replace with the  desired ppn\n",
    "startdate =  \"1970-01-01\" ## Example date, please replace with the  desired start date, use \"\" (empty string) for all years\n",
    "enddate = \"1970-12-31\" ## Example date, please replace with the  desired end date, use \"\" (empty string) for all years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the newspaper issue identifiers\n",
    "\n",
    "Before we can download the actual content, we need a list of identifiers from the newspapers that fit in the selection  we made above. We put this list in a dataframe in which we store some additional metadata  about the newspaper. This  dataframe is used later on for accessing the content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We start by creating the sru queries based on the parameters above\n",
    "def sru_query(ppn, startdate, enddate, startRecord, batchSize = 1000):\n",
    "    if startdate == \"\" and enddate == \"\":\n",
    "        sru_query = f\"http://jsru.kb.nl/sru/sru/{apikey}?operations=searchRetrieve&recordSchema=ddd&x-collection=DDD_krantnr\"\\\n",
    "            f\"&query=(ppn={ppn})\"\\\n",
    "            f\"&startRecord={startRecord}&maximumRecords={batchSize}\"\n",
    "    else:\n",
    "        sru_query = f\"http://jsru.kb.nl/sru/sru/{apikey}?operations=searchRetrieve&recordSchema=ddd&x-collection=DDD_krantnr\"\\\n",
    "            f\"&query=(date%20within%20%22{startdate}%20{enddate}%22)%20and%20(ppn={ppn})\"\\\n",
    "            f\"&startRecord={startRecord}&maximumRecords={batchSize}\" \n",
    "    return(sru_query)\n",
    "\n",
    "## Request metadata from SRU as specified in query and return processed XML (beautiful soup)\n",
    "def sru_request(query):\n",
    "    page = requests.get(query)\n",
    "    soup = BeautifulSoup(page.content, \"xml\")\n",
    "    return soup\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, we extract the total number of newspapers that were found. \n",
    "## We need this to iterate through the sru untill all files are downloaded\n",
    "soup = sru_request(sru_query(ppn, startdate, enddate, startRecord = 0, batchSize = 0))\n",
    "for item in soup.findAll('srw:searchRetrieveResponse'):\n",
    "    nRecords = 5#int(item.find('srw:numberOfRecords').text)\n",
    "    \n",
    "## Create an empty dataframe to store the identifiers and some metadata\n",
    "dfIdentifiers = pd.DataFrame(columns = ['identifier', 'papertitle', 'date'])\n",
    "\n",
    "## Then we iterate through the sru, extract the required data and put it in the dataframe\n",
    "## After each loop, increase 'startRecord' to access the next batch of newpaper identifiers.  \n",
    "startRecord = 0\n",
    "batchSize = 2\n",
    "while startRecord <= nRecords:\n",
    "    soup = sru_request(sru_query(ppn, startdate, enddate, startRecord,batchSize = batchSize))\n",
    "    identifiers, papertitles, dates = [],[],[]\n",
    "    for item in soup.findAll('srw:recordData'):\n",
    "        identifiers.append(item.find('ddd:metadataKey').text)\n",
    "        papertitles.append(item.find('ddd:publisher').text)\n",
    "        dates.append(item.find('dc:date').text)\n",
    "    batch = pd.DataFrame({'identifier': identifiers, 'papertitle': papertitles, 'date': dates})\n",
    "    dfIdentifiers = pd.concat([dfIdentifiers,batch], ignore_index = True)\n",
    "    startRecord += batchSize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show the first 4 records of the dataframe to see what we got\n",
    "dfIdentifiers.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## And show how many newspapers were found\n",
    "print('Number of newspaper issues: ' + str(len(dfIdentifiers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the desired items\n",
    "\n",
    "You can recieve each newspaper in several formats:\n",
    "* The image of a page (jp2 format, high quality)\n",
    "* The complete  newspaper in pdf (lower quality)\n",
    "* The alto xml of a page\n",
    "* The metadata of the complete newspaper \n",
    "* The plain text of every article in the newspaper\n",
    "\n",
    "First, you have to define the function 'download_file' (see next cell). Then, you can choose which of the above types you want to download and run the corresponding cells beneath to collect the data. Please note: depending on the amount of newspapers, this can take a while. \n",
    "\n",
    "NB: make sure the folder you are referring to in your path (see cells below) does exist before running the code. If the folder does not exist, the code will stop running and gives an error. \n",
    "\n",
    "**Explanation of the download code** <br>\n",
    "You iterate through the dataframe and perform the following steps for each newspaper:\n",
    "* Retrieve the identifier.\n",
    "* Add or change information from the identifier if needed (see the cells: download image, alto and metadata).\n",
    "* Create a filename out of the identifier, with characters that are allowed for filenames (e.g. replace the colon in the identifier name with a underscore).\n",
    "* Create the url for quering the sru of oai. \n",
    "* Run the function \"download_file\" to download the content from the url. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for downloading a file from a url\n",
    "def download_file(download_url, filename):\n",
    "    ## Retrieve the desired information from the given url\n",
    "    response = urllib.request.urlopen(download_url)\n",
    "    # TODO: check for errors\n",
    "    ## Save to the desired location\n",
    "    with open(filename, 'wb') as f:\n",
    "        f. write(response. read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download image (jp2 format)\n",
    "\n",
    "NB: the jp2 files have large filesizes and not all image manipulation software can open these files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"images\" ## Change to the desired location\n",
    "if not os.path.exists(path):\n",
    "   os.makedirs(path)\n",
    "pagenumber = '001' ##  Choose the pagenumber you want to retrieve, NB: pagenumbers are always three digits \n",
    "\n",
    "for index, row in dfIdentifiers.iterrows():\n",
    "    identifier = row['identifier']\n",
    "    prefix = identifier.split(\"=\")[1].split(\":\")[0]\n",
    "    if prefix == 'ddd':\n",
    "        identifier = identifier + \":p\"+pagenumber\n",
    "    else: \n",
    "        identifier = identifier.replace(\"mpeg21\", pagenumber)\n",
    "    filename = identifier.split(\"=\")[1].replace(\":\",\"_\")\n",
    "    url = identifier + \":image\"\n",
    "    download_file(url, os.path.join(path, filename+ \".jp2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"pdfs\" ## Change to the desired location\n",
    "if not os.path.exists(path):\n",
    "   os.makedirs(path)\n",
    "\n",
    "for index, row in dfIdentifiers.iterrows():\n",
    "    identifier = row['identifier']\n",
    "    filename = identifier.split(\"=\")[1].replace(\":\",\"_\")\n",
    "    url = identifier + \":pdf\"\n",
    "    download_file(url, os.path.join(path, filename+ \".pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download alto xml of page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"alto\" ## Change to the desired location\n",
    "if not os.path.exists(path):\n",
    "   os.makedirs(path)\n",
    "\n",
    "pagenumber = '001' ##  Choose the pagenumber you want to retrieve, NB: pagenumbers are always three digits \n",
    "\n",
    "for index, row in dfIdentifiers.iterrows():\n",
    "    identifier = row['identifier']\n",
    "    prefix = identifier.split(\"=\")[1]\n",
    "    prefix = prefix.split(\":\",1)[0]\n",
    "    if prefix == 'ddd':\n",
    "        identifier = identifier + \":p\"+pagenumber\n",
    "    else:\n",
    "        identifier = identifier.replace(\"mpeg21\", pagenumber)\n",
    "    filename = identifier.split(\"=\")[1].replace(\":\",\"_\")\n",
    "    url = identifier + \":alto\"\n",
    "    download_file(url, os.path.join(path, filename+ \".xml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"metadata\" ## Change to the desired location\n",
    "apikey=\"ab5a8969-b339-4d3b-a76a-636342f71e55\"\n",
    "if not os.path.exists(path):\n",
    "   os.makedirs(path)\n",
    "for index, row in dfIdentifiers.iterrows():\n",
    "    identifier = row['identifier']\n",
    "    identifier = identifier.split(\"=\")[1]\n",
    "    prefix = identifier.split(\":\",1)[0]\n",
    "    identifier = identifier.split(\":\",1)[1]\n",
    "    filename = identifier.replace(\":\",\"_\")\n",
    "    if prefix == 'ddd':\n",
    "        url = f\"http://services.kb.nl/mdo/oai/{apikey}?verb=GetRecord&\" \\\n",
    "              f\"identifier=DDD:ddd:{identifier}&metadataPrefix=didl\"\n",
    "    if prefix == 'ABCDDD':\n",
    "        url = f\"http://services.kb.nl/mdo/oai/{apikey}?verb=GetRecord&\" \\\n",
    "              f\"identifier=KRANTEN:DDD:ddd:{identifier}&metadataPrefix=didl\"\n",
    "    else:\n",
    "        url = f\"http://services.kb.nl/mdo/oai/{apikey}?verb=GetRecord&\" \\\n",
    "              f\"identifier=KRANTEN:{prefix}:{prefix}:{identifier}&metadataPrefix=didl\"\n",
    "    download_file(url, os.path.join(path, filename+ \".xml\"))\n",
    "    # TODO: check that the request succeeded, i.e. you were authorized to view the record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download plain text from articles\n",
    "\n",
    "For this step, you first need to download the metadata files and store them on your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Collect all the identifiers for the individual articles. These are used later on for downloading the text\n",
    "\n",
    "metadata_path = \"metadata\" ## Change to the path where you saved the metadata files\n",
    "\n",
    "def obtain_article_identifiers(metadatafile):\n",
    "    with open(os.path.join(path,filename), \"r\", encoding=\"utf8\") as f:\n",
    "            content = \"\".join(f.readlines())\n",
    "            soup = BeautifulSoup(content, \"xml\")\n",
    "    identifiers = []\n",
    "    for identifier in soup.findAll('dc:identifier'):\n",
    "        identifier = identifier.text\n",
    "        if identifier.startswith(\"http://resolver.kb.nl/\"):\n",
    "            if 'a' in identifier.rsplit(\":\", 1)[1]: \n",
    "                identifiers.append(identifier)\n",
    "    return identifiers\n",
    "\n",
    "def obtain_article_content(identifier_ocr):\n",
    "    r = requests.get(identifier_ocr)\n",
    "    soup = BeautifulSoup(r.content, \"xml\")\n",
    "    try:\n",
    "        title = soup.find('title').text\n",
    "    except AttributeError: \n",
    "        print(f'Item without title: {identifier_ocr}')\n",
    "        title = ''\n",
    "    content = ' '.join([item.text for item in soup.findAll('p')])\n",
    "    return title, content\n",
    "\n",
    "## Create empty dataframe to contain article content\n",
    "dfArticleContent = pd.DataFrame(columns = ['identifier', 'title', 'content'])\n",
    "i=0\n",
    "\n",
    "## Iterate over the metadata files. Retrieve article identifiers and obtain corresponding contents\n",
    "for (_, _, filenames) in os.walk(metadata_path):\n",
    "    for filename in [fn for fn in filenames if fn.endswith('.xml')]:\n",
    "        for article_identifier in obtain_article_identifiers(filename):\n",
    "            title, content = obtain_article_content(article_identifier+':ocr')\n",
    "            dfArticleContent.loc[i] = pd.Series({'identifier': article_identifier, 'title': title, 'content':  content})\n",
    "            i+=1\n",
    "\n",
    "            \n",
    "## Show the first records of the dataframe to see what we got\n",
    "dfArticleContent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the dataframe for further use\n",
    "dataframe_path = 'Insert pathname' #\"insert a path to store the dataframe\"\n",
    "dataframe_name = \"Insert filename\" # insert the desired filename\n",
    "\n",
    "# Save the dataframe as comma seperated file\n",
    "dfArticleContent.to_csv(dataframe_path + \"/\" + dataframe_name + \".csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}